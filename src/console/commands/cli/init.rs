use std::convert::TryFrom;
use std::path::{Path, PathBuf};

use crate::cli::ai_client::{AiProvider, create_provider};
use crate::cli::ai_scanner::generate_config_with_ai;
use crate::cli::config_parser::{
    AiConfig, AiProviderType, AppType, ConfigBuilder, DomainConfig, ProxyConfig, ProxyType,
    SslMode, StackerConfig,
};
use crate::cli::detector::{detect_project, RealFileSystem};
use crate::cli::error::CliError;
use crate::cli::generator::compose::ComposeDefinition;
use crate::cli::generator::dockerfile::DockerfileBuilder;
use crate::console::commands::CallableTrait;

/// Default config filename generated by `stacker init`.
pub const DEFAULT_CONFIG_FILE: &str = "stacker.yml";

pub fn full_config_reference_example() -> &'static str {
    r#"# -----------------------------------------------------------------------------
# FULL COMMENTED REFERENCE (examples)
# Uncomment and adapt the sections you need.
# -----------------------------------------------------------------------------
# organization: "acme-inc"
# env_file: ".env"
# env:
#   LOG_LEVEL: "info"
#
# app:
#   # type: static | node | python | rust | go | php | custom
#   type: "node"
#   path: "."
#   # dockerfile: "Dockerfile"
#   # image: "ghcr.io/your-org/your-image:latest"  # usually for type: custom
#   # build:
#   #   context: "."
#   #   args:
#   #     APP_ENV: "production"
#
# services:
#   - name: "postgres"
#     image: "postgres:16"
#     ports: ["5432:5432"]
#     environment:
#       POSTGRES_DB: "app"
#       POSTGRES_USER: "app"
#       POSTGRES_PASSWORD: "change-me"
#     volumes:
#       - "postgres_data:/var/lib/postgresql/data"
#     depends_on: []
#
# proxy:
#   # type: none | nginx | traefik
#   type: "nginx"
#   auto_detect: true
#   domains:
#     - domain: "app.example.com"
#       # ssl: none | auto | manual
#       ssl: "auto"
#       upstream: "app:3000"
#   # config: "./nginx.conf"
#
# deploy:
#   # target: local | cloud | server
#   target: "cloud"
#   # compose_file: "docker-compose.yml"
#   cloud:
#     # provider: hetzner | digitalocean | aws | linode | vultr
#     provider: "hetzner"
#     region: "nbg1"
#     size: "cx22"
#     # ssh_key: "~/.ssh/id_rsa"
#   # server:
#   #   host: "203.0.113.10"
#   #   user: "root"
#   #   port: 22
#   #   ssh_key: "~/.ssh/id_rsa"
#
# ai:
#   enabled: true
#   # provider: openai | anthropic | ollama | custom
#   provider: "ollama"
#   # model: "deepseek-r1:latest"
#   # api_key: "${OPENAI_API_KEY}"
#   # endpoint: "http://localhost:11434"
#   timeout: 300
#   tasks: ["dockerfile", "compose"]
#
# monitoring:
#   status_panel: true
#   # healthcheck:
#   #   endpoint: "/health"
#   #   interval: "30s"
#   # metrics:
#   #   enabled: true
#   #   telegraf: true
#
# hooks:
#   # pre_build: "./scripts/pre-build.sh"
#   # post_deploy: "./scripts/post-deploy.sh"
#   # on_failure: "./scripts/on-failure.sh"
# -----------------------------------------------------------------------------"#
}

/// `stacker init [--type static|node|python|rust|go|php] [--with-proxy] [--with-ai]`
///
/// Detects the project type in the current directory and generates
/// a `stacker.yml` configuration file with sensible defaults.
///
/// When `--with-ai` is used with an AI provider configured (via flags
/// or environment variables), Stacker scans the project deeply and
/// sends the context to the AI to generate a tailored `stacker.yml`
/// with appropriate services, proxy, monitoring, etc.
pub struct InitCommand {
    pub app_type: Option<String>,
    pub with_proxy: bool,
    pub with_ai: bool,
    /// Override AI provider (openai, anthropic, ollama, custom)
    pub ai_provider: Option<String>,
    /// Override AI model name
    pub ai_model: Option<String>,
    /// Override AI API key
    pub ai_api_key: Option<String>,
}

impl InitCommand {
    pub fn new(app_type: Option<String>, with_proxy: bool, with_ai: bool) -> Self {
        Self {
            app_type,
            with_proxy,
            with_ai,
            ai_provider: None,
            ai_model: None,
            ai_api_key: None,
        }
    }

    pub fn with_ai_options(
        mut self,
        provider: Option<String>,
        model: Option<String>,
        api_key: Option<String>,
    ) -> Self {
        self.ai_provider = provider;
        self.ai_model = model;
        self.ai_api_key = api_key;
        self
    }
}

/// Parse an app type string (e.g. "node", "static") into `AppType`.
fn parse_app_type(s: &str) -> Result<AppType, CliError> {
    let json = format!("\"{}\"", s.to_lowercase());
    serde_json::from_str::<AppType>(&json).map_err(|_| {
        CliError::ConfigValidation(format!(
            "Unknown app type '{}'. Valid types: static, node, python, rust, go, php, custom",
            s
        ))
    })
}

/// Build an `AiConfig` from CLI flags and/or environment variables.
///
/// Priority: CLI flag > environment variable > defaults.
pub fn resolve_ai_config(
    ai_provider: Option<&str>,
    ai_model: Option<&str>,
    ai_api_key: Option<&str>,
) -> Result<AiConfig, CliError> {
    // Provider: flag > env > default (ollama)
    let provider_str = ai_provider
        .map(|s| s.to_string())
        .or_else(|| std::env::var("STACKER_AI_PROVIDER").ok())
        .unwrap_or_else(|| "ollama".to_string());

    let provider = parse_ai_provider(&provider_str)?;

    // API key: flag > env (provider-specific) > env (generic) > None
    let api_key = ai_api_key
        .map(|s| s.to_string())
        .or_else(|| match provider {
            AiProviderType::Openai => std::env::var("OPENAI_API_KEY").ok(),
            AiProviderType::Anthropic => std::env::var("ANTHROPIC_API_KEY").ok(),
            _ => None,
        })
        .or_else(|| std::env::var("STACKER_AI_API_KEY").ok());

    // Model: flag > env > None (provider default will be used)
    let model = ai_model
        .map(|s| s.to_string())
        .or_else(|| std::env::var("STACKER_AI_MODEL").ok());

    // Endpoint from env
    let endpoint = std::env::var("STACKER_AI_ENDPOINT").ok();

    // Timeout: env > default (300s)
    let timeout = std::env::var("STACKER_AI_TIMEOUT")
        .ok()
        .and_then(|v| v.parse::<u64>().ok())
        .unwrap_or(300);

    Ok(AiConfig {
        enabled: true,
        provider,
        model,
        api_key,
        endpoint,
        timeout,
        tasks: vec!["dockerfile".to_string(), "compose".to_string()],
    })
}

/// Parse an AI provider string into `AiProviderType`.
fn parse_ai_provider(s: &str) -> Result<AiProviderType, CliError> {
    match s.to_lowercase().as_str() {
        "openai" => Ok(AiProviderType::Openai),
        "anthropic" => Ok(AiProviderType::Anthropic),
        "ollama" => Ok(AiProviderType::Ollama),
        "custom" => Ok(AiProviderType::Custom),
        other => Err(CliError::ConfigValidation(format!(
            "Unknown AI provider '{}'. Valid: openai, anthropic, ollama, custom",
            other
        ))),
    }
}

/// Generate a `stacker.yml` config in the target directory.
///
/// This is extracted as a standalone function for testability â€” the
/// `InitCommand::call()` delegates here with `std::env::current_dir()`.
///
/// NOTE: This function always uses template-based generation (no network calls).
/// For AI-powered generation, use `generate_config_full()` with explicit
/// provider options â€” that path is invoked by the real CLI binary.
pub fn generate_config(
    project_dir: &Path,
    app_type_override: Option<&str>,
    with_proxy: bool,
    with_ai: bool,
) -> Result<PathBuf, CliError> {
    let config_path = project_dir.join(DEFAULT_CONFIG_FILE);

    if config_path.exists() {
        return Err(CliError::ConfigValidation(format!(
            "{} already exists. Remove it first or edit it directly.",
            DEFAULT_CONFIG_FILE
        )));
    }

    generate_config_template_path(
        project_dir,
        &config_path,
        app_type_override,
        with_proxy,
        with_ai,
    )
}

/// Full config generation supporting AI provider options.
pub fn generate_config_full(
    project_dir: &Path,
    app_type_override: Option<&str>,
    with_proxy: bool,
    with_ai: bool,
    ai_provider: Option<&str>,
    ai_model: Option<&str>,
    ai_api_key: Option<&str>,
) -> Result<PathBuf, CliError> {
    let config_path = project_dir.join(DEFAULT_CONFIG_FILE);

    // Don't overwrite existing config
    if config_path.exists() {
        return Err(CliError::ConfigValidation(format!(
            "{} already exists. Remove it first or edit it directly.",
            DEFAULT_CONFIG_FILE
        )));
    }

    // If --with-ai is set, try AI-powered generation
    if with_ai {
        let ai_config = resolve_ai_config(ai_provider, ai_model, ai_api_key)?;

        match create_provider(&ai_config) {
            Ok(provider) => {
                match generate_config_ai_path(
                    project_dir,
                    &config_path,
                    provider.as_ref(),
                    &ai_config,
                ) {
                    Ok(path) => return Ok(path),
                    Err(e) => {
                        eprintln!("\nâš  AI generation failed: {}", e);
                        eprintln!("  Falling back to template-based generation.");
                        eprintln!("  Tip: make sure your Ollama model supports code generation.");
                        eprintln!("  Available models can be listed with: ollama list\n");
                    }
                }
            }
            Err(e) => {
                eprintln!("\nâš  AI provider not available: {}", e);
                eprintln!("  Falling back to template-based generation.");
                eprintln!("  Tip: start Ollama with: ollama serve\n");
            }
        }
    }

    // Template-based generation (original flow)
    generate_config_template_path(
        project_dir,
        &config_path,
        app_type_override,
        with_proxy,
        with_ai,
    )
}

/// AI-powered config generation path.
fn generate_config_ai_path(
    project_dir: &Path,
    config_path: &Path,
    provider: &dyn AiProvider,
    ai_config: &AiConfig,
) -> Result<PathBuf, CliError> {
    eprintln!("ðŸ¤– Scanning project and generating config with AI...");

    let yaml = generate_config_with_ai(project_dir, provider)?;

    // Validate it parses as StackerConfig
    match StackerConfig::from_str(&yaml) {
        Ok(_) => {}
        Err(e) => {
            // Save the raw AI output for debugging but warn
            let debug_path = project_dir.join("stacker.yml.ai-draft");
            let _ = std::fs::write(&debug_path, &yaml);
            return Err(CliError::ConfigValidation(format!(
                "AI generated a config that failed validation: {}. \
                 Raw output saved to stacker.yml.ai-draft for review.",
                e
            )));
        }
    }

    // Write with header
    let content = format!(
        "# Stacker configuration â€” generated by `stacker init --with-ai`\n\
         # AI provider: {} (model: {})\n\
         # Review this file and adjust as needed before deploying.\n\
         # Docs: https://docs.try.direct/stacker\n\
         \n\
         {yaml}\n\
         \n\
         {}\n",
        ai_config.provider,
        ai_config
            .model
            .as_deref()
            .unwrap_or("default"),
        full_config_reference_example(),
    );

    std::fs::write(config_path, &content)?;

    Ok(config_path.to_path_buf())
}

/// Template-based config generation (the original flow).
fn generate_config_template_path(
    project_dir: &Path,
    config_path: &Path,
    app_type_override: Option<&str>,
    with_proxy: bool,
    with_ai: bool,
) -> Result<PathBuf, CliError> {
    // Determine app type: flag override > detection > default (static)
    let app_type = if let Some(type_str) = app_type_override {
        parse_app_type(type_str)?
    } else {
        let fs = RealFileSystem;
        let detection = detect_project(project_dir, &fs);
        detection.app_type
    };

    // Derive project name from directory name
    let project_name = project_dir
        .file_name()
        .and_then(|n| n.to_str())
        .unwrap_or("my-app")
        .to_string();

    // Build config
    let mut builder = ConfigBuilder::new()
        .name(&project_name)
        .version("0.1.0")
        .app_type(app_type)
        .app_path(".");

    if with_proxy {
        builder = builder.proxy(ProxyConfig {
            proxy_type: ProxyType::Nginx,
            domains: vec![DomainConfig {
                domain: format!("{}.localhost", project_name),
                ssl: SslMode::Auto,
                upstream: "app:80".to_string(),
            }],
            ..ProxyConfig::default()
        });
    }

    if with_ai {
        builder = builder.ai(AiConfig {
            enabled: true,
            provider: AiProviderType::Ollama,
            ..AiConfig::default()
        });
    }

    let config = builder.build()?;

    // Serialize to YAML
    let yaml = serde_yaml::to_string(&config)
        .map_err(|e| CliError::GeneratorError(format!("Failed to serialize config: {e}")))?;

    // Write with a header comment
    let content = format!(
        "# Stacker configuration â€” generated by `stacker init`\n\
         # Docs: https://docs.try.direct/stacker\n\
         \n\
         {yaml}"
    );

    std::fs::write(config_path, &content)?;

    Ok(config_path.to_path_buf())
}

/// Output directory for generated artifacts.
const OUTPUT_DIR: &str = ".stacker";

impl CallableTrait for InitCommand {
    fn call(&self) -> Result<(), Box<dyn std::error::Error>> {
        let project_dir = std::env::current_dir()?;

        let config_path = generate_config_full(
            &project_dir,
            self.app_type.as_deref(),
            self.with_proxy,
            self.with_ai,
            self.ai_provider.as_deref(),
            self.ai_model.as_deref(),
            self.ai_api_key.as_deref(),
        )?;

        eprintln!("âœ“ Created {}", config_path.display());

        // Verify the generated file is parseable
        let config = StackerConfig::from_file(&config_path)?;
        eprintln!("  Project: {} ({})", config.name, config.app.app_type);

        if self.with_proxy || config.proxy.proxy_type != ProxyType::None {
            eprintln!("  Proxy: enabled ({})", config.proxy.proxy_type);
        }
        if config.ai.enabled {
            eprintln!("  AI: enabled ({})", config.ai.provider);
        }
        if !config.services.is_empty() {
            eprintln!("  Services: {}", config.services.iter().map(|s| s.name.as_str()).collect::<Vec<_>>().join(", "));
        }

        // Generate .stacker/ directory with Dockerfile and docker-compose.yml
        let output_dir = project_dir.join(OUTPUT_DIR);
        std::fs::create_dir_all(&output_dir)?;

        // Generate Dockerfile (skip if user specified a custom one or an image)
        let needs_dockerfile = config.app.image.is_none() && config.app.dockerfile.is_none();
        if needs_dockerfile {
            let dockerfile_path = output_dir.join("Dockerfile");
            let builder = DockerfileBuilder::from(config.app.app_type);
            builder.write_to(&dockerfile_path, false)?;
            eprintln!("âœ“ Generated {}/Dockerfile", OUTPUT_DIR);
        }

        // Generate docker-compose.yml (skip if user specified an existing compose file)
        if config.deploy.compose_file.is_none() {
            let compose_path = output_dir.join("docker-compose.yml");
            let compose = ComposeDefinition::try_from(&config)?;
            compose.write_to(&compose_path, false)?;
            eprintln!("âœ“ Generated {}/docker-compose.yml", OUTPUT_DIR);
        }

        eprintln!("\nNext steps:");
        eprintln!("  stacker config validate   # Check configuration");
        eprintln!("  stacker deploy --target local --dry-run   # Preview deployment");
        eprintln!("  stacker deploy --target local   # Deploy locally");

        Ok(())
    }
}

// â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
// Tests
// â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    fn setup_dir_with_files(files: &[&str]) -> TempDir {
        let dir = TempDir::new().unwrap();
        for f in files {
            std::fs::write(dir.path().join(f), "").unwrap();
        }
        dir
    }

    #[test]
    fn test_init_static_project_creates_config() {
        let dir = setup_dir_with_files(&["index.html"]);
        let result = generate_config(dir.path(), None, false, false);
        assert!(result.is_ok());

        let path = result.unwrap();
        assert!(path.exists());

        let config = StackerConfig::from_file(&path).unwrap();
        assert_eq!(config.app.app_type, AppType::Static);
    }

    #[test]
    fn test_init_node_project_detects_correctly() {
        let dir = setup_dir_with_files(&["package.json"]);
        let result = generate_config(dir.path(), None, false, false);
        assert!(result.is_ok());

        let config = StackerConfig::from_file(&result.unwrap()).unwrap();
        assert_eq!(config.app.app_type, AppType::Node);
    }

    #[test]
    fn test_init_type_flag_overrides_detection() {
        // Dir has package.json (Node) but flag says python
        let dir = setup_dir_with_files(&["package.json"]);
        let result = generate_config(dir.path(), Some("python"), false, false);
        assert!(result.is_ok());

        let config = StackerConfig::from_file(&result.unwrap()).unwrap();
        assert_eq!(config.app.app_type, AppType::Python);
    }

    #[test]
    fn test_init_with_proxy_flag_adds_section() {
        let dir = setup_dir_with_files(&["index.html"]);
        let result = generate_config(dir.path(), None, true, false);
        assert!(result.is_ok());

        let config = StackerConfig::from_file(&result.unwrap()).unwrap();
        assert_eq!(config.proxy.proxy_type, ProxyType::Nginx);
        assert!(!config.proxy.domains.is_empty());
        assert!(config.proxy.domains[0].domain.contains("localhost"));
    }

    #[test]
    fn test_init_with_ai_flag_adds_section() {
        let dir = setup_dir_with_files(&["index.html"]);
        let result = generate_config(dir.path(), None, false, true);
        assert!(result.is_ok());

        let config = StackerConfig::from_file(&result.unwrap()).unwrap();
        assert!(config.ai.enabled);
        assert_eq!(config.ai.provider, AiProviderType::Ollama);
    }

    #[test]
    fn test_init_does_not_overwrite_existing() {
        let dir = setup_dir_with_files(&["index.html", DEFAULT_CONFIG_FILE]);
        let result = generate_config(dir.path(), None, false, false);
        assert!(result.is_err());

        let err = format!("{}", result.unwrap_err());
        assert!(err.contains("already exists"));
    }

    #[test]
    fn test_init_output_parses_as_valid_config() {
        let dir = setup_dir_with_files(&["index.html"]);
        let path = generate_config(dir.path(), None, false, false).unwrap();

        // Must be parseable
        let result = StackerConfig::from_file(&path);
        assert!(result.is_ok());
    }

    #[test]
    fn test_init_empty_dir_defaults_to_custom() {
        let dir = TempDir::new().unwrap();
        let result = generate_config(dir.path(), None, false, false);
        assert!(result.is_ok());

        let config = StackerConfig::from_file(&result.unwrap()).unwrap();
        // Empty dir with no recognized files â†’ Custom (detector default)
        assert_eq!(config.app.app_type, AppType::Custom);
    }

    #[test]
    fn test_parse_app_type_valid() {
        assert_eq!(parse_app_type("static").unwrap(), AppType::Static);
        assert_eq!(parse_app_type("node").unwrap(), AppType::Node);
        assert_eq!(parse_app_type("Python").unwrap(), AppType::Python);
        assert_eq!(parse_app_type("RUST").unwrap(), AppType::Rust);
    }

    #[test]
    fn test_parse_app_type_invalid() {
        let result = parse_app_type("java");
        assert!(result.is_err());
        let err = format!("{}", result.unwrap_err());
        assert!(err.contains("Unknown app type"));
    }

    // â”€â”€ AI provider resolution tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    #[test]
    fn test_resolve_ai_config_defaults_to_ollama() {
        let config = resolve_ai_config(None, None, None).unwrap();
        assert!(config.enabled);
        assert_eq!(config.provider, AiProviderType::Ollama);
        assert!(config.api_key.is_none());
    }

    #[test]
    fn test_resolve_ai_config_explicit_provider() {
        let config = resolve_ai_config(Some("anthropic"), Some("claude-sonnet-4-20250514"), Some("sk-ant-test")).unwrap();
        assert_eq!(config.provider, AiProviderType::Anthropic);
        assert_eq!(config.model.as_deref(), Some("claude-sonnet-4-20250514"));
        assert_eq!(config.api_key.as_deref(), Some("sk-ant-test"));
    }

    #[test]
    fn test_resolve_ai_config_openai_with_key() {
        let config = resolve_ai_config(Some("openai"), None, Some("sk-test123")).unwrap();
        assert_eq!(config.provider, AiProviderType::Openai);
        assert_eq!(config.api_key.as_deref(), Some("sk-test123"));
    }

    #[test]
    fn test_resolve_ai_config_invalid_provider_errors() {
        let result = resolve_ai_config(Some("invalid-provider"), None, None);
        assert!(result.is_err());
        let err = format!("{}", result.unwrap_err());
        assert!(err.contains("Unknown AI provider"));
    }

    #[test]
    fn test_resolve_ai_config_env_var_fallback() {
        std::env::set_var("STACKER_AI_PROVIDER", "openai");
        std::env::set_var("OPENAI_API_KEY", "sk-from-env");
        std::env::set_var("STACKER_AI_MODEL", "gpt-4o-mini");

        let config = resolve_ai_config(None, None, None).unwrap();
        assert_eq!(config.provider, AiProviderType::Openai);
        assert_eq!(config.api_key.as_deref(), Some("sk-from-env"));
        assert_eq!(config.model.as_deref(), Some("gpt-4o-mini"));

        std::env::remove_var("STACKER_AI_PROVIDER");
        std::env::remove_var("OPENAI_API_KEY");
        std::env::remove_var("STACKER_AI_MODEL");
    }

    #[test]
    fn test_resolve_ai_config_flag_overrides_env() {
        std::env::set_var("STACKER_AI_PROVIDER", "openai");

        // Flag says ollama, env says openai â€” flag wins
        let config = resolve_ai_config(Some("ollama"), None, None).unwrap();
        assert_eq!(config.provider, AiProviderType::Ollama);

        std::env::remove_var("STACKER_AI_PROVIDER");
    }

    #[test]
    fn test_resolve_ai_config_timeout_default() {
        std::env::remove_var("STACKER_AI_TIMEOUT");
        let config = resolve_ai_config(None, None, None).unwrap();
        assert_eq!(config.timeout, 300);
    }

    #[test]
    fn test_resolve_ai_config_timeout_from_env() {
        std::env::set_var("STACKER_AI_TIMEOUT", "900");
        let config = resolve_ai_config(None, None, None).unwrap();
        assert_eq!(config.timeout, 900);
        std::env::remove_var("STACKER_AI_TIMEOUT");
    }

    #[test]
    fn test_parse_ai_provider_all_valid() {
        assert_eq!(parse_ai_provider("openai").unwrap(), AiProviderType::Openai);
        assert_eq!(parse_ai_provider("anthropic").unwrap(), AiProviderType::Anthropic);
        assert_eq!(parse_ai_provider("ollama").unwrap(), AiProviderType::Ollama);
        assert_eq!(parse_ai_provider("custom").unwrap(), AiProviderType::Custom);
        // Case insensitive
        assert_eq!(parse_ai_provider("OpenAI").unwrap(), AiProviderType::Openai);
        assert_eq!(parse_ai_provider("ANTHROPIC").unwrap(), AiProviderType::Anthropic);
    }

    #[test]
    fn test_generate_config_full_template_fallback() {
        // with_ai=true but bogus provider â†’ create_provider fails â†’ falls back to template
        let dir = setup_dir_with_files(&["package.json"]);
        // Use an explicit provider that will fail connection (port 1 is unreachable)
        // This avoids hitting a real running Ollama instance
        let result = generate_config_full(
            dir.path(), None, false, true,
            Some("custom"), None, Some("fake-key"),
        );
        assert!(result.is_ok());

        let config = StackerConfig::from_file(&result.unwrap()).unwrap();
        // Template fallback still generates correct app type
        assert_eq!(config.app.app_type, AppType::Node);
        // And includes AI section in config
        assert!(config.ai.enabled);
    }
}
